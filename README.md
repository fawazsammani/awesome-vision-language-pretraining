# Awesome Vision-Language Pretraining [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

Pretraining is blowing the field of Vision-Language Research! Let's keep track on all the works before it gets too late! Papers not based on pretraining can be found in other awesomes linked at the end of the repo. 

If you find some overlooked papers, please open issues or pull requests, and provide the paper(s) in this format:
```
- **[]** Paper Name [[pdf]]() [[code]]()
```

## Papers

- **[ViLBERT]** Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks [[pdf]](https://arxiv.org/pdf/1908.02265.pdf) [[code]](https://github.com/facebookresearch/vilbert-multi-task) [[code]](https://github.com/jiasenlu/vilbert_beta)
- **[ImageBERT]** Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data [[pdf]](https://arxiv.org/pdf/2001.07966.pdf)
- **[SimVLM]** Simple Visual Language Model Pretraining with Weak Supervision [[pdf]](https://arxiv.org/pdf/2108.10904.pdf)
- **[ALBEF]** Align before Fuse: Vision and Language Representation Learning with Momentum Distillation [[pdf]](https://arxiv.org/pdf/2107.07651.pdf) [[code]](https://github.com/salesforce/ALBEF)
- **[LXMERT]** Learning Cross-Modality Encoder Representations from Transformers [[pdf]](https://arxiv.org/pdf/1908.07490.pdf) [[code]](https://github.com/airsplay/lxmert)
- **[X-LXMERT]** Paint, Caption and Answer Questions with Multi-Modal Transformers [[pdf]](https://arxiv.org/pdf/2009.11278.pdf) [[code]](https://github.com/allenai/x-lxmert)
- **[VisualBERT]** A Simple and Performant Baseline for Vision and Language [[pdf]](https://arxiv.org/pdf/1908.03557.pdf)
- **[UNIMO]** Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning [[pdf]](https://arxiv.org/pdf/2012.15409.pdf) [[code]](https://github.com/PaddlePaddle/Research/tree/master/NLP/UNIMO)
- **[UNIMO-2]** End-to-End Unified Vision-Language Grounded Learning [[pdf]](https://arxiv.org/pdf/2203.09067.pdf) [[code]](https://unimo-ptm.github.io/)
- **[BLIP]** Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation [[pdf]](https://arxiv.org/pdf/2201.12086.pdf) [[code]](https://github.com/salesforce/BLIP) [[video]](https://www.youtube.com/watch?v=X2k7n4FuI7c&ab_channel=YannicKilcher)
- **[Uni-EDEN]** Universal Encoder-Decoder Network by Multi-Granular Vision-Language Pre-training [[pdf]](https://arxiv.org/pdf/2201.04026.pdf)
- **[VisualGPT]** Data-efficient Adaptation of Pretrained Language Models for Image Captioning [[pdf]](https://arxiv.org/pdf/2102.10407.pdf) [[code]](https://github.com/Vision-CAIR/VisualGPT)
- **[XGPT]** Cross-modal Generative Pre-Training for Image Captioning [[pdf]](https://arxiv.org/pdf/2003.01473.pdf)
- **[ViTCAP]** Injecting Semantic Concepts into End-to-End Image Captioning [[pdf]](https://arxiv.org/pdf/2112.05230.pdf) 
- **[LEMON]** Scaling Up Vision-Language Pre-training for Image Captioning [[pdf]](https://arxiv.org/pdf/2111.12233.pdf) 
- **[Unified-VLP]** Unified Vision-Language Pre-Training for Image Captioning and VQA [[pdf]](https://arxiv.org/pdf/1909.11059.pdf) [[code]](https://github.com/LuoweiZhou/VLP)
- **[TAP]** Text-Aware Pre-training for Text-VQA and Text-Caption [[pdf]](https://arxiv.org/pdf/2012.04638.pdf) 
- **[PICa]** An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA [[pdf]](https://arxiv.org/pdf/2109.05014.pdf) 
- **[CVLP]** Contrastive Visual-Linguistic Pretraining [[pdf]](https://arxiv.org/pdf/2007.13135.pdf) [[code]](https://github.com/ArcherYunDong/CVLP)
- **[UniT]** Multimodal Multitask Learning with a Unified Transformer [[pdf]](https://arxiv.org/pdf/2102.10772.pdf) [[website]](https://mmf.sh/)
- **[VL-BERT]** Pre-training of Generic Visual-Linguistic Representations [[pdf]](https://arxiv.org/pdf/1908.08530.pdf) [[code]](https://github.com/jackroos/VL-BERT)
- **[Unicoder-VL]** A Universal Encoder for Vision and Language by Cross-modal Pre-training [[pdf]](https://arxiv.org/pdf/1908.06066.pdf) 
- **[UNITER]** UNiversal Image-TExt Representation Learning [[pdf]](https://arxiv.org/pdf/1909.11740.pdf) [[code]](https://github.com/ChenRocks/UNITER)
- **[ViLT]** Vision-and-Language Transformer Without Convolution or Region Supervision [[pdf]](https://arxiv.org/pdf/2102.03334.pdf) [[code]](https://github.com/dandelin/vilt)
- **[GLIP ]** Grounded Language-Image Pre-training [[pdf]](https://arxiv.org/pdf/2112.03857.pdf) [[code]](https://github.com/microsoft/GLIP)
- **[VLMo]** Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts [[pdf]](https://arxiv.org/pdf/2111.02358.pdf) [[code]](https://github.com/microsoft/unilm/tree/master/vlmo)
- **[METER]** An Empirical Study of Training End-to-End Vision-and-Language Transformers [[pdf]](https://arxiv.org/pdf/2111.02387.pdf) [[code]](https://github.com/zdou0830/METER)
- **[WenLan]** Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training [[pdf]](https://arxiv.org/pdf/2103.06561.pdf)
- **[InterBERT]** Vision-and-Language Interaction for Multi-modal Pretraining [[pdf]](https://arxiv.org/pdf/2003.13198.pdf)
- **[SemVLP]** Vision-Language Pre-training by Aligning Semantics at Multiple Levels [[pdf]](https://arxiv.org/pdf/2103.07829.pdf)
- **[E2E-VLP]** End-to-End Vision-Language Pre-training Enhanced by Visual Learning [[pdf]](https://arxiv.org/pdf/2106.01804.pdf) 
- **[VinVL]** Revisiting Visual Representations in Vision-Language Models [[pdf]](https://arxiv.org/pdf/2101.00529.pdf) [[code]](https://github.com/microsoft/Oscar) [[code]](https://github.com/pzzhang/VinVL)
- **[UFO]** A UniFied TransfOrmer for Vision-Language Representation Learning [[pdf]](https://arxiv.org/pdf/2111.10023.pdf)
- **[Florence]** A New Foundation Model for Computer Vision [[pdf]](https://arxiv.org/pdf/2111.11432.pdf)
- **[VILLA]** Large-Scale Adversarial Training for Vision-and-Language Representation Learning [[pdf]](https://arxiv.org/pdf/2006.06195.pdf) [[code]](https://github.com/zhegan27/VILLA)
- **[TDEN]** Scheduled Sampling in Vision-Language Pretraining with Decoupled Encoder-Decoder Network [[pdf]](https://arxiv.org/pdf/2101.11562.pdf) [[code]](https://github.com/YehLi/TDEN)
- **[ERNIE-ViL]** Knowledge Enhanced Vision-Language Representations Through Scene Graph [[pdf]](https://arxiv.org/pdf/2006.16934.pdf)
- **[Vokenization]** Improving Language Understanding with Contextualized, Visual-Grounded Supervision [[pdf]](https://arxiv.org/pdf/2010.06775.pdf) [[code]](https://github.com/airsplay/vokenization)
- **[12-in-1]** Multi-Task Vision and Language Representation Learning [[pdf]](https://arxiv.org/pdf/1912.02315.pdf) [[code]](https://github.com/facebookresearch/vilbert-multi-task)
- **[KVL-BERT]** Knowledge Enhanced Visual-and-Linguistic BERT for Visual Commonsense Reasoning [[pdf]](https://arxiv.org/pdf/2012.07000.pdf)
- **[Oscar]** Object-Semantics Aligned Pre-training for Vision-Language Tasks [[pdf]](https://arxiv.org/pdf/2004.06165.pdf) [[code]](https://github.com/microsoft/Oscar)
- **[VIVO]** Visual Vocabulary Pre-Training for Novel Object Captioning [[pdf]](https://arxiv.org/pdf/2009.13682.pdf)
- **[SOHO]** End-to-End Pre-training for Vision-Language Representation Learning [[pdf]](https://arxiv.org/pdf/2104.03135.pdf) [[code]](https://github.com/researchmm/soho)
- **[Pixel-BERT]** Aligning Image Pixels with Text by Deep Multi-Modal Transformers [[pdf]](https://arxiv.org/pdf/2004.00849.pdf)
- **[ImageGPT]** Generative Pretraining from Pixels [[pdf]](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf) [[code]](https://github.com/openai/image-gpt) [[code]](https://github.com/karpathy/minGPT) [[website]](https://openai.com/blog/image-gpt/)
- **[CLIP-ViL]** How Much Can CLIP Benefit Vision-and-Language Tasks? [[pdf]](https://arxiv.org/pdf/2107.06383.pdf) [[code]](https://github.com/clip-vil/CLIP-ViL)
- **[VLKD]** Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation [[pdf]](https://arxiv.org/pdf/2203.06386.pdf)
- **[LightningDOT]** Pre-training Visual-Semantic Embeddings for Real-Time Image-Text Retrieval [[pdf]](https://arxiv.org/pdf/2103.08784.pdf) [[code]](https://github.com/intersun/LightningDOT)
- **[VirTex]** Learning Visual Representations from Textual Annotations [[pdf]](https://arxiv.org/pdf/2006.06666.pdf) [[code]](https://github.com/kdexd/virtex)
- **[CoOp]** Learning to Prompt for Vision-Language Models [[pdf]](https://arxiv.org/pdf/2109.01134.pdf) [[code]](https://github.com/KaiyangZhou/CoOp)
- **[CoCoOp]** Conditional Prompt Learning for Vision-Language Models [[pdf]](https://arxiv.org/pdf/2203.05557.pdf) [[code]](https://github.com/KaiyangZhou/CoOp)
- **[Uni-Perceiver]** Pre-training Unified Architecture for Generic Perception for Zero-shot and Few-shot Tasks [[pdf]](https://arxiv.org/pdf/2112.01522.pdf)
- **[CoCa]** Contrastive Captioners are Image-Text Foundation Models [[pdf]](https://arxiv.org/pdf/2205.01917.pdf) [[code]](https://github.com/lucidrains/CoCa-pytorch)
- **[Flamingo]** A Visual Language Model for Few-Shot Learning [[pdf]](https://arxiv.org/pdf/2204.14198.pdf) [[code]](https://github.com/lucidrains/flamingo-pytorch) [[website]](https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model)
- **[UniCL]** Unified Contrastive Learning in Image-Text-Label Space [[pdf]](https://arxiv.org/pdf/2204.03610.pdf) [[code]](https://github.com/microsoft/UniCL)
- **[UVLP]** Unsupervised Vision-and-Language Pre-training via Retrieval-based Multi-Granular Alignment [[pdf]](https://arxiv.org/pdf/2203.00242.pdf)
- **[OFA]** Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework [[pdf]](https://arxiv.org/pdf/2202.03052v2.pdf) [[code]](https://github.com/OFA-Sys/OFA)
- **[GPV-1]** Towards General Purpose Vision Systems: An End-to-End Task-Agnostic Vision-Language Architecture [[pdf]](https://arxiv.org/pdf/2104.00743.pdf) [[code]](https://github.com/allenai/gpv-1/) [[website]](https://prior.allenai.org/projects/gpv)
- **[TCL]** Vision-Language Pre-Training with Triple Contrastive Learning [[pdf]](https://arxiv.org/pdf/2202.10401.pdf) [[code]](https://github.com/uta-smile/TCL)
- **[L-Verse]** Bidirectional Generation Between Image and Text [[pdf]](https://arxiv.org/pdf/2111.11133.pdf)
- **[FLAVA]** A Foundational Language And Vision Alignment Model [[pdf]](https://arxiv.org/pdf/2112.04482.pdf) [[website]](https://flava-model.github.io/)
- **[COTS]** Collaborative Two-Stream Vision-Language Pre-Training Model for Cross-Modal Retrieval [[pdf]](https://arxiv.org/pdf/2204.07441.pdf)
- **[VL-ADAPTER]** Parameter-Efficient Transfer Learning for Vision-and-Language Tasks [[pdf]](https://arxiv.org/pdf/2112.06825.pdf) [[code]](https://github.com/ylsung/VL_adapter)
- **[mPLUG]** Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections [[pdf]](https://arxiv.org/pdf/2205.12005.pdf) 
- **[DALL-E]** Zero-Shot Text-to-Image Generation [[pdf]](https://arxiv.org/pdf/2102.12092.pdf) [[code]](https://github.com/openai/DALL-E) [[code]](https://github.com/borisdayma/dalle-mini) [[code]](https://github.com/lucidrains/DALLE-pytorch) [[code]](https://github.com/robvanvolt/DALLE-models) [[code]](https://github.com/kakaobrain/minDALL-E) [[video]](https://www.youtube.com/watch?v=j4xgkjWlfL4&t=1432s&ab_channel=YannicKilcher) [[video]](https://www.youtube.com/watch?v=jMqLTPcA9CQ&t=1034s&ab_channel=TheAIEpiphany) [[blog]](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-mini--Vmlldzo4NjIxODA) [[blog]](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-mini-Generate-images-from-any-text-prompt--VmlldzoyMDE4NDAy) [[blog]](https://ml.berkeley.edu/blog/posts/vq-vae/) [[website]](https://openai.com/blog/dall-e/)
- **[DALL-E-2]** Hierarchical Text-Conditional Image Generation with CLIP Latents [[pdf]](https://arxiv.org/pdf/2204.06125.pdf) [[code]](https://github.com/lucidrains/DALLE2-pytorch) [[website]](https://openai.com/dall-e-2/)
- **[Make-A-Scene]** Scene-Based Text-to-Image Generation with Human Priors [[pdf]](https://arxiv.org/pdf/2203.13131.pdf)
- **[PaLM]** Scaling Language Modeling with Pathways [[pdf]](https://arxiv.org/pdf/2204.02311.pdf) [[blog]](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html) [[code]](https://github.com/lucidrains/PaLM-pytorch)
- **[VL-BEiT]** Generative Vision-Language Pretraining [[pdf]](https://arxiv.org/pdf/2206.01127.pdf) [[code]](https://github.com/microsoft/unilm/tree/master/vl-beit)
- **[MetaLM]** Language Models are General-Purpose Interfaces [[pdf]](https://arxiv.org/pdf/2206.06336.pdf) [[code]](https://github.com/microsoft/unilm/tree/master/metalm)
- **[VL-T5]** Unifying Vision-and-Language Tasks via Text Generation [[pdf]](https://arxiv.org/pdf/2102.02779.pdf) [[code]](https://github.com/j-min/VL-T5)
- **[UNICORN]** Crossing the Format Boundary of Text and Boxes: Towards Unified Vision-Language Modeling [[pdf]](https://arxiv.org/pdf/2111.12085.pdf)
- **[MI2P]** Expanding Large Pre-trained Unimodal Models with Multimodal Information Injection for Image-Text Multimodal Classification [[pdf]](https://openaccess.thecvf.com/content/CVPR2022/papers/Liang_Expanding_Large_Pre-Trained_Unimodal_Models_With_Multimodal_Information_Injection_for_CVPR_2022_paper.pdf)
- Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers [[pdf]](https://arxiv.org/pdf/2109.04448.pdf)
- A Closer Look at the Robustness of Vision-and-Language Pre-trained Models [[pdf]](https://arxiv.org/pdf/2012.08673.pdf)
- Probing Inter-modality: Visual Parsing with Self-Attention for Vision-Language Pre-training [[pdf]](https://arxiv.org/pdf/2106.13488.pdf) 
- Multimodal Pretraining Unmasked: A Meta-Analysis and a Unified Framework of Vision-and-Language BERTs [[pdf]](https://arxiv.org/pdf/2011.15124.pdf) 
- Unsupervised Vision-and-Language Pre-training Without Parallel Images and Captions [[pdf]](https://arxiv.org/pdf/2010.12831.pdf)
- Cross-Modal Textual and Visual Context for Image Captioning [[pdf]](https://arxiv.org/pdf/2205.04363.pdf)
- Multi-modal Alignment using Representation Codebook [[pdf]](https://arxiv.org/pdf/2203.00048.pdf)
- A Closer Look at the Robustness of Vision-and-Language Pre-trained Models [[pdf]](https://arxiv.org/pdf/2012.08673.pdf)
- CLIP Models are Few-shot Learners: Empirical Studies on VQA and Visual Entailment [[pdf]](https://arxiv.org/pdf/2203.07190.pdf)
- Multimodal Few-Shot Learning with Frozen Language Models [[pdf]](https://arxiv.org/pdf/2106.13884.pdf)

## CLIP and Improvements
- **[CLIP]** Learning Transferable Visual Models From Natural Language Supervision [[pdf]](https://arxiv.org/pdf/2103.00020.pdf) [[code]](https://github.com/openai/CLIP) [[code]](https://github.com/mlfoundations/open_clip) [[code]](https://github.com/Zasder3/train-CLIP) [[website]](https://openai.com/blog/clip/) [[video]](https://www.youtube.com/watch?v=T9XSU0pKX2E&t=1455s&ab_channel=YannicKilcher)
- **[ALIGN]** Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision [[pdf]](https://arxiv.org/pdf/2102.05918.pdf)
- **[DeCLIP]** Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm [[pdf]](https://arxiv.org/pdf/2110.05208.pdf) [[code]](https://github.com/Sense-GVT/DeCLIP)
- **[LiT]** Zero-Shot Transfer with Locked-image text Tuning [[pdf]](https://arxiv.org/pdf/2111.07991.pdf) [[code]](https://github.com/google-research/vision_transformer) [[website]](https://google-research.github.io/vision_transformer/lit/)
- **[RegionCLIP]** Region-based Language-Image Pretraining [[pdf]](https://arxiv.org/pdf/2112.09106.pdf) [[code]](https://github.com/microsoft/RegionCLIP)
- **[DenseCLIP]** Language-Guided Dense Prediction with Context-Aware Prompting [[pdf]](https://arxiv.org/pdf/2112.01518.pdf) [[code]](https://github.com/raoyongming/DenseCLIP)
- **[X-CLIP]** Variants of CLIP [[code]](https://github.com/lucidrains/x-clip)
- **[CLIP Retrieval]** [[blog]](https://rom1504.medium.com/semantic-search-with-embeddings-index-anything-8fb18556443c) [[code]](https://github.com/rom1504/clip-retrieval)
- **[E-CLIP]** Efficient Cross-Modal Pre-training by Ensemble Confident Learning and Language Modeling [[pdf]](https://arxiv.org/pdf/2109.04699.pdf)
- Robust Cross-Modal Representation Learning with Progressive Self-Distillation [[pdf]](https://arxiv.org/pdf/2204.04588.pdf)
- Disentangling visual and written concepts in CLIP [[pdf]](https://arxiv.org/pdf/2206.07835.pdf) [[code]](https://github.com/joaanna/disentangling_spelling_in_clip) [[website]](https://joaanna.github.io/disentangling_spelling_in_clip/)
- [awesome-clip](https://github.com/yzhuoning/Awesome-CLIP)

## Diffusion-based
- CLIP-Guided Diffusion [[code]](https://github.com/openai/guided-diffusion) [[code]](https://github.com/afiaka87/clip-guided-diffusion) [[code]](https://github.com/nerdyrodent/CLIP-Guided-Diffusion) [[code]](https://github.com/crowsonkb/v-diffusion-pytorch)
- **[LDMs]** High-Resolution Image Synthesis with Latent Diffusion Models [[pdf]](https://arxiv.org/pdf/2112.10752.pdf) [[code]](https://github.com/CompVis/latent-diffusion)
- **[GLIDE]** Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models [[pdf]](https://arxiv.org/pdf/2112.10741.pdf) [[code]](https://github.com/openai/glide-text2im) [[video]](https://www.youtube.com/watch?v=gwI6g1pBD84&t=1976s&ab_channel=YannicKilcher) [[video]](https://www.youtube.com/watch?v=lvv4N2nf-HU&t=1891s&ab_channel=TheAIEpiphany) [[video]](https://www.youtube.com/watch?v=344w5h24-h8&t=5s&ab_channel=AICoffeeBreakwithLetitia)
- **[DiffusionCLIP]** Text-Guided Diffusion Models for Robust Image Manipulation [[pdf]](https://arxiv.org/pdf/2110.02711.pdf) [[code]](https://github.com/gwang-kim/DiffusionCLIP)
- **[Imagen]** Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding [[pdf]](https://arxiv.org/pdf/2205.11487.pdf) [[code]](https://github.com/lucidrains/imagen-pytorch) [[code]](https://github.com/cene555/Imagen-pytorch) [[website]](https://imagen.research.google/)

## New Large-Scale Datasets
- **[VisualCOMET]** Reasoning about the Dynamic Context of a Still Image [[pdf]](https://arxiv.org/pdf/2004.10796.pdf) [[website]](https://visualcomet.xyz/)
- **[LAION]** [[website]](https://laion.ai/#top)
- **[Conceptual 12M]** Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts [[pdf]](https://arxiv.org/pdf/2102.08981.pdf) [[code]](https://github.com/google-research-datasets/conceptual-12m)
- **[Winoground]** Probing Vision and Language Models for Visio-Linguistic Compositionality [[pdf]](https://arxiv.org/pdf/2204.03162.pdf) [[dataset]](https://huggingface.co/datasets/facebook/winoground)

### Libraries
- [diffusers](https://github.com/huggingface/diffusers)
- [X-modaler](https://github.com/YehLi/xmodaler)
- [Transformers-VQA](https://github.com/YIKUAN8/Transformers-VQA)
- [MMT-Retrieval](https://github.com/UKPLab/MMT-Retrieval)
- [MMF](https://github.com/facebookresearch/mmf)

### Projects
- [Florence-VL](https://www.microsoft.com/en-us/research/project/project-florence-vl/)
- [unilm](https://github.com/microsoft/unilm)

### Other Awesomes
- [awesome-Vision-and-Language-Pre-training](https://github.com/phellonchen/awesome-Vision-and-Language-Pre-training)
- [awesome-vision-language-pretraining-papers](https://github.com/yuewang-cuhk/awesome-vision-language-pretraining-papers)
- [vqa](https://github.com/jokieleung/awesome-visual-question-answering)
- [image captioning](https://github.com/forence/Awesome-Visual-Captioning)
- [image captioning](https://github.com/zhjohnchan/awesome-image-captioning)
- [scene graphs](https://github.com/huoxingmeishi/Awesome-Scene-Graphs)

### Surveys
- A Survey of Vision-Language Pre-Trained Models [[pdf]](https://arxiv.org/pdf/2202.10936.pdf)
- VLP: A Survey on Vision-Language Pre-training [[pdf]](https://arxiv.org/pdf/2202.09061.pdf)
- Vision Language models: towards multi-modal deep learning [[blog]](https://theaisummer.com/vision-language-models/)

