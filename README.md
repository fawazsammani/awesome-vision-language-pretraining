# Awesome Vision-Language Pretraining [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

Pretraining is blowing the field of Vision-Language Research! Let's keep track on all the works before it gets too late! Papers not based on pretraining can be found in other awesomes linked at the end of the repo. 

If you find some overlooked papers, please open issues or pull requests, and provide the paper(s) in this format:
```
- **[]** Paper Name [[pdf]]() [[code]]()
```

Note: most pretrained models can be found on [hf models](https://huggingface.co/models)

## Papers

- **[ViLBERT]** Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks [[pdf]](https://arxiv.org/pdf/1908.02265.pdf) [[code]](https://github.com/facebookresearch/vilbert-multi-task) [[code]](https://github.com/jiasenlu/vilbert_beta)
- **[ImageBERT]** Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data [[pdf]](https://arxiv.org/pdf/2001.07966.pdf)
- **[SimVLM]** Simple Visual Language Model Pretraining with Weak Supervision [[pdf]](https://arxiv.org/pdf/2108.10904.pdf)
- **[ALBEF]** Align before Fuse: Vision and Language Representation Learning with Momentum Distillation [[pdf]](https://arxiv.org/pdf/2107.07651.pdf) [[code]](https://github.com/salesforce/ALBEF)
- **[LXMERT]** Learning Cross-Modality Encoder Representations from Transformers [[pdf]](https://arxiv.org/pdf/1908.07490.pdf) [[code]](https://github.com/airsplay/lxmert)
- **[X-LXMERT]** Paint, Caption and Answer Questions with Multi-Modal Transformers [[pdf]](https://arxiv.org/pdf/2009.11278.pdf) [[code]](https://github.com/allenai/x-lxmert)
- **[VisualBERT]** A Simple and Performant Baseline for Vision and Language [[pdf]](https://arxiv.org/pdf/1908.03557.pdf)
- **[UNIMO]** Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning [[pdf]](https://arxiv.org/pdf/2012.15409.pdf) [[code]](https://github.com/PaddlePaddle/Research/tree/master/NLP/UNIMO)
- **[UNIMO-2]** End-to-End Unified Vision-Language Grounded Learning [[pdf]](https://arxiv.org/pdf/2203.09067.pdf) [[code]](https://unimo-ptm.github.io/)
- **[BLIP]** Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation [[pdf]](https://arxiv.org/pdf/2201.12086.pdf) [[code]](https://github.com/salesforce/BLIP) [[video]](https://www.youtube.com/watch?v=X2k7n4FuI7c&ab_channel=YannicKilcher)
- **[BLIP-2]** Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models [[pdf]](https://arxiv.org/pdf/2301.12597.pdf) [[code]](https://github.com/salesforce/LAVIS/tree/main/projects/blip2) [[blog]](https://huggingface.co/blog/blip-2)
- **[Uni-EDEN]** Universal Encoder-Decoder Network by Multi-Granular Vision-Language Pre-training [[pdf]](https://arxiv.org/pdf/2201.04026.pdf)
- **[VisualGPT]** Data-efficient Adaptation of Pretrained Language Models for Image Captioning [[pdf]](https://arxiv.org/pdf/2102.10407.pdf) [[code]](https://github.com/Vision-CAIR/VisualGPT)
- **[MiniVLM]** A Smaller and Faster Vision-Language Model [[pdf]](https://arxiv.org/pdf/2012.06946.pdf)
- **[XGPT]** Cross-modal Generative Pre-Training for Image Captioning [[pdf]](https://arxiv.org/pdf/2003.01473.pdf)
- **[ViTCAP]** Injecting Semantic Concepts into End-to-End Image Captioning [[pdf]](https://arxiv.org/pdf/2112.05230.pdf) 
- **[LEMON]** Scaling Up Vision-Language Pre-training for Image Captioning [[pdf]](https://arxiv.org/pdf/2111.12233.pdf) 
- **[Unified-VLP]** Unified Vision-Language Pre-Training for Image Captioning and VQA [[pdf]](https://arxiv.org/pdf/1909.11059.pdf) [[code]](https://github.com/LuoweiZhou/VLP)
- **[TAP]** Text-Aware Pre-training for Text-VQA and Text-Caption [[pdf]](https://arxiv.org/pdf/2012.04638.pdf) 
- **[PICa]** An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA [[pdf]](https://arxiv.org/pdf/2109.05014.pdf) 
- **[CVLP]** Contrastive Visual-Linguistic Pretraining [[pdf]](https://arxiv.org/pdf/2007.13135.pdf) [[code]](https://github.com/ArcherYunDong/CVLP)
- **[UniT]** Multimodal Multitask Learning with a Unified Transformer [[pdf]](https://arxiv.org/pdf/2102.10772.pdf) [[website]](https://mmf.sh/)
- **[VL-BERT]** Pre-training of Generic Visual-Linguistic Representations [[pdf]](https://arxiv.org/pdf/1908.08530.pdf) [[code]](https://github.com/jackroos/VL-BERT)
- **[Unicoder-VL]** A Universal Encoder for Vision and Language by Cross-modal Pre-training [[pdf]](https://arxiv.org/pdf/1908.06066.pdf) 
- **[UNITER]** UNiversal Image-TExt Representation Learning [[pdf]](https://arxiv.org/pdf/1909.11740.pdf) [[code]](https://github.com/ChenRocks/UNITER)
- **[ViLT]** Vision-and-Language Transformer Without Convolution or Region Supervision [[pdf]](https://arxiv.org/pdf/2102.03334.pdf) [[code]](https://github.com/dandelin/ViLT) [[demo]](https://huggingface.co/spaces/nielsr/vilt-vqa)
- **[GLIP ]** Grounded Language-Image Pre-training [[pdf]](https://arxiv.org/pdf/2112.03857.pdf) [[code]](https://github.com/microsoft/GLIP)
- **[GLIPv2]** Unifying Localization and Vision-Language Understanding [[pdf]](https://arxiv.org/pdf/2206.05836.pdf) [[code]](https://github.com/microsoft/GLIP)
- **[VLMo]** Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts [[pdf]](https://arxiv.org/pdf/2111.02358.pdf) [[code]](https://github.com/microsoft/unilm/tree/master/vlmo)
- **[METER]** An Empirical Study of Training End-to-End Vision-and-Language Transformers [[pdf]](https://arxiv.org/pdf/2111.02387.pdf) [[code]](https://github.com/zdou0830/METER)
- **[WenLan]** Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training [[pdf]](https://arxiv.org/pdf/2103.06561.pdf)
- **[InterBERT]** Vision-and-Language Interaction for Multi-modal Pretraining [[pdf]](https://arxiv.org/pdf/2003.13198.pdf)
- **[SemVLP]** Vision-Language Pre-training by Aligning Semantics at Multiple Levels [[pdf]](https://arxiv.org/pdf/2103.07829.pdf)
- **[E2E-VLP]** End-to-End Vision-Language Pre-training Enhanced by Visual Learning [[pdf]](https://arxiv.org/pdf/2106.01804.pdf) 
- **[VinVL]** Revisiting Visual Representations in Vision-Language Models [[pdf]](https://arxiv.org/pdf/2101.00529.pdf) [[code]](https://github.com/microsoft/Oscar) [[code]](https://github.com/pzzhang/VinVL)
- **[UFO]** A UniFied TransfOrmer for Vision-Language Representation Learning [[pdf]](https://arxiv.org/pdf/2111.10023.pdf)
- **[Florence]** A New Foundation Model for Computer Vision [[pdf]](https://arxiv.org/pdf/2111.11432.pdf)
- **[VILLA]** Large-Scale Adversarial Training for Vision-and-Language Representation Learning [[pdf]](https://arxiv.org/pdf/2006.06195.pdf) [[code]](https://github.com/zhegan27/VILLA)
- **[TDEN]** Scheduled Sampling in Vision-Language Pretraining with Decoupled Encoder-Decoder Network [[pdf]](https://arxiv.org/pdf/2101.11562.pdf) [[code]](https://github.com/YehLi/TDEN)
- **[ERNIE-ViL]** Knowledge Enhanced Vision-Language Representations Through Scene Graph [[pdf]](https://arxiv.org/pdf/2006.16934.pdf)
- **[Vokenization]** Improving Language Understanding with Contextualized, Visual-Grounded Supervision [[pdf]](https://arxiv.org/pdf/2010.06775.pdf) [[code]](https://github.com/airsplay/vokenization)
- **[12-in-1]** Multi-Task Vision and Language Representation Learning [[pdf]](https://arxiv.org/pdf/1912.02315.pdf) [[code]](https://github.com/facebookresearch/vilbert-multi-task)
- **[KVL-BERT]** Knowledge Enhanced Visual-and-Linguistic BERT for Visual Commonsense Reasoning [[pdf]](https://arxiv.org/pdf/2012.07000.pdf)
- **[Oscar]** Object-Semantics Aligned Pre-training for Vision-Language Tasks [[pdf]](https://arxiv.org/pdf/2004.06165.pdf) [[code]](https://github.com/microsoft/Oscar)
- **[VIVO]** Visual Vocabulary Pre-Training for Novel Object Captioning [[pdf]](https://arxiv.org/pdf/2009.13682.pdf)
- **[SOHO]** End-to-End Pre-training for Vision-Language Representation Learning [[pdf]](https://arxiv.org/pdf/2104.03135.pdf) [[code]](https://github.com/researchmm/soho)
- **[Pixel-BERT]** Aligning Image Pixels with Text by Deep Multi-Modal Transformers [[pdf]](https://arxiv.org/pdf/2004.00849.pdf)
- **[VLKD]** Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation [[pdf]](https://arxiv.org/pdf/2203.06386.pdf)
- **[LightningDOT]** Pre-training Visual-Semantic Embeddings for Real-Time Image-Text Retrieval [[pdf]](https://arxiv.org/pdf/2103.08784.pdf) [[code]](https://github.com/intersun/LightningDOT)
- **[VirTex]** Learning Visual Representations from Textual Annotations [[pdf]](https://arxiv.org/pdf/2006.06666.pdf) [[code]](https://github.com/kdexd/virtex)
- **[CoOp]** Learning to Prompt for Vision-Language Models [[pdf]](https://arxiv.org/pdf/2109.01134.pdf) [[code]](https://github.com/KaiyangZhou/CoOp)
- **[CoCoOp]** Conditional Prompt Learning for Vision-Language Models [[pdf]](https://arxiv.org/pdf/2203.05557.pdf) [[code]](https://github.com/KaiyangZhou/CoOp)
- **[Uni-Perceiver]** Pre-training Unified Architecture for Generic Perception for Zero-shot and Few-shot Tasks [[pdf]](https://arxiv.org/pdf/2112.01522.pdf) [[code]](https://github.com/fundamentalvision/Uni-Perceiver)
- **[Uni-Perceiver v2]** A Generalist Model for Large-Scale Vision and Vision-Language Tasks [[pdf]](https://arxiv.org/pdf/2211.09808.pdf) [[code]](https://github.com/fundamentalvision/Uni-Perceiver)
- **[CoCa]** Contrastive Captioners are Image-Text Foundation Models [[pdf]](https://arxiv.org/pdf/2205.01917.pdf) [[code]](https://github.com/lucidrains/CoCa-pytorch) [[code]](https://github.com/mlfoundations/open_clip) [[colab]](https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_coca.ipynb) 
- **[Flamingo]** A Visual Language Model for Few-Shot Learning [[pdf]](https://arxiv.org/pdf/2204.14198.pdf) [[code]](https://github.com/lucidrains/flamingo-pytorch) [[website]](https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model) [[blog]](https://wandb.ai/gladiator/Flamingo%20VLM/reports/DeepMind-Flamingo-A-Visual-Language-Model-for-Few-Shot-Learning--VmlldzoyOTgzMDI2)
- **[BEiT-3]** Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks [[pdf]](https://arxiv.org/pdf/2208.10442.pdf) [[code]](https://github.com/microsoft/unilm/tree/master/beit3)
- **[UniCL]** Unified Contrastive Learning in Image-Text-Label Space [[pdf]](https://arxiv.org/pdf/2204.03610.pdf) [[code]](https://github.com/microsoft/UniCL)
- **[UVLP]** Unsupervised Vision-and-Language Pre-training via Retrieval-based Multi-Granular Alignment [[pdf]](https://arxiv.org/pdf/2203.00242.pdf)
- **[OFA]** Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework [[pdf]](https://arxiv.org/pdf/2202.03052.pdf) [[code]](https://github.com/OFA-Sys/OFA)
- **[GPV-1]** Towards General Purpose Vision Systems: An End-to-End Task-Agnostic Vision-Language Architecture [[pdf]](https://arxiv.org/pdf/2104.00743.pdf) [[code]](https://github.com/allenai/gpv-1/) [[website]](https://prior.allenai.org/projects/gpv)
- **[GPV-2]** Webly Supervised Concept Expansion for General Purpose Vision Models [[pdf]](https://arxiv.org/pdf/2202.02317.pdf) [[code]](https://github.com/allenai/gpv2/) [[website]](https://prior.allenai.org/projects/gpv2)
- **[TCL]** Vision-Language Pre-Training with Triple Contrastive Learning [[pdf]](https://arxiv.org/pdf/2202.10401.pdf) [[code]](https://github.com/uta-smile/TCL)
- **[L-Verse]** Bidirectional Generation Between Image and Text [[pdf]](https://arxiv.org/pdf/2111.11133.pdf)
- **[FLAVA]** A Foundational Language And Vision Alignment Model [[pdf]](https://arxiv.org/pdf/2112.04482.pdf) [[code]](https://github.com/facebookresearch/multimodal/tree/main/examples/flava) [[website]](https://flava-model.github.io/) [[tutorial]](https://pytorch.org/tutorials/beginner/flava_finetuning_tutorial.html)
- **[COTS]** Collaborative Two-Stream Vision-Language Pre-Training Model for Cross-Modal Retrieval [[pdf]](https://arxiv.org/pdf/2204.07441.pdf)
- **[VL-ADAPTER]** Parameter-Efficient Transfer Learning for Vision-and-Language Tasks [[pdf]](https://arxiv.org/pdf/2112.06825.pdf) [[code]](https://github.com/ylsung/VL_adapter)
- **[Unified-IO]** A Unified Model for Vision, Language, and Multi-Modal Tasks [[pdf]](https://arxiv.org/pdf/2206.08916.pdf) [[website]](https://unified-io.allenai.org/)
- **[X-VLM]** Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts [[pdf]](https://arxiv.org/pdf/2111.08276v3.pdf) [[code]](https://github.com/zengyan-97/x-vlm)
- **[ZeroCap]** Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic [[pdf]](https://arxiv.org/pdf/2111.14447.pdf) [[code]](https://github.com/YoadTew/zero-shot-image-to-text)
- **[FewVLM]** A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models [[pdf]](https://arxiv.org/pdf/2110.08484.pdf) [[code]](https://github.com/woojeongjin/FewVLM)
- **[mPLUG]** Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections [[pdf]](https://arxiv.org/pdf/2205.12005v2.pdf) 
- **[PaLI]** A Jointly-Scaled Multilingual Language-Image Model [[pdf]](https://arxiv.org/pdf/2209.06794v2.pdf) [[blog]](https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html)
- **[GIT]** A Generative Image-to-text Transformer for Vision and Language [[pdf]](https://arxiv.org/pdf/2205.14100.pdf) [[code]](https://github.com/microsoft/GenerativeImage2Text)
- **[MaskVLM]** Masked Vision and Language Modeling for Multi-modal Representation Learning [[pdf]](https://arxiv.org/pdf/2208.02131.pdf)
- **[DALL-E]** Zero-Shot Text-to-Image Generation [[pdf]](https://arxiv.org/pdf/2102.12092.pdf) [[code]](https://github.com/openai/DALL-E) [[code]](https://github.com/borisdayma/dalle-mini) [[code]](https://github.com/lucidrains/DALLE-pytorch) [[code]](https://github.com/kuprel/min-dalle) [[code]](https://github.com/robvanvolt/DALLE-models) [[code]](https://github.com/kakaobrain/minDALL-E) [[website]](https://openai.com/blog/dall-e/) [[video]](https://www.youtube.com/watch?v=j4xgkjWlfL4&t=1432s&ab_channel=YannicKilcher) [[video]](https://www.youtube.com/watch?v=jMqLTPcA9CQ&t=1034s&ab_channel=TheAIEpiphany) [[video]](https://www.youtube.com/watch?v=x_8uHX5KngE&ab_channel=TheAIEpiphany) [[blog]](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-mini--Vmlldzo4NjIxODA) [[blog]](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-mini-Generate-images-from-any-text-prompt--VmlldzoyMDE4NDAy) [[blog]](https://wandb.ai/dalle-mini/dalle-mini/reports/Building-efficient-image-input-pipelines--VmlldzoyMjMxOTQw) [[blog]](https://ml.berkeley.edu/blog/posts/vq-vae/) [[blog]](https://ml.berkeley.edu/blog/posts/dalle2/) [[blog]](https://towardsdatascience.com/understanding-how-dall-e-mini-works-114048912b3b)
- **[Make-A-Scene]** Scene-Based Text-to-Image Generation with Human Priors [[pdf]](https://arxiv.org/pdf/2203.13131.pdf)
- **[Make-A-Video]** Text-to-Video Generation without Text-Video Data [[pdf]](https://arxiv.org/pdf/2209.14792.pdf) [[code]](https://github.com/lucidrains/make-a-video-pytorch) [[blog]](https://makeavideo.studio/) [[blog]](https://ai.facebook.com/blog/generative-ai-text-to-video/) [[video]](https://www.youtube.com/watch?v=AcvmyqGgMh8&ab_channel=AICoffeeBreakwithLetitia) [[video]](https://www.youtube.com/watch?v=MmAJk2BD6WA)
- **[GIT]** A Generative Image-to-text Transformer for Vision and Language [[pdf]](https://arxiv.org/pdf/2205.14100.pdf)
- **[FIBER]** Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone [[pdf]](https://arxiv.org/pdf/2206.07643.pdf) [[code]](https://github.com/microsoft/FIBER)
- **[VL-BEiT]** Generative Vision-Language Pretraining [[pdf]](https://arxiv.org/pdf/2206.01127.pdf) [[code]](https://github.com/microsoft/unilm/tree/master/vl-beit)
- **[MetaLM]** Language Models are General-Purpose Interfaces [[pdf]](https://arxiv.org/pdf/2206.06336.pdf) [[code]](https://github.com/microsoft/unilm/tree/master/metalm)
- **[VL-T5]** Unifying Vision-and-Language Tasks via Text Generation [[pdf]](https://arxiv.org/pdf/2102.02779.pdf) [[code]](https://github.com/j-min/VL-T5)
- **[UNICORN]** Crossing the Format Boundary of Text and Boxes: Towards Unified Vision-Language Modeling [[pdf]](https://arxiv.org/pdf/2111.12085.pdf)
- **[MI2P]** Expanding Large Pre-trained Unimodal Models with Multimodal Information Injection for Image-Text Multimodal Classification [[pdf]](https://openaccess.thecvf.com/content/CVPR2022/papers/Liang_Expanding_Large_Pre-Trained_Unimodal_Models_With_Multimodal_Information_Injection_for_CVPR_2022_paper.pdf)
- **[MDETR]** Modulated Detection for End-to-End Multi-Modal Understanding [[pdf]](https://arxiv.org/pdf/2104.12763.pdf) [[code]](https://github.com/ashkamath/mdetr)
- **[VLMixer]** Unpaired Vision-Language Pre-training via Cross-Modal CutMix [[pdf]](https://arxiv.org/pdf/2206.08919.pdf) [[code]](https://github.com/ttengwang/VLMixer)
- **[ViCHA]** Efficient Vision-Language Pretraining with Visual Concepts and Hierarchical Alignment [[pdf]](https://arxiv.org/pdf/2208.13628.pdf) [[code]](https://github.com/mshukor/ViCHA)
- **[Parti]** Scaling Autoregressive Models for Content-Rich Text-to-Image Generation [[pdf]](https://arxiv.org/pdf/2206.10789.pdf) [[code]](https://github.com/google-research/parti) [[code]](https://github.com/lucidrains/parti-pytorch) [[video]](https://www.youtube.com/watch?v=qS-iYnp00uc&ab_channel=YannicKilcher) [[blog]](https://parti.research.google/)
- **[StoryDALL-E]** Adapting Pretrained Text-to-Image Transformers for Story Continuation [[pdf]](https://arxiv.org/pdf/2209.06192v1.pdf) [[code]](https://github.com/adymaharana/storydalle)
- **[GRIT]** Faster and Better Image captioning Transformer Using Dual Visual Features [[pdf]](https://arxiv.org/pdf/2207.09666v1.pdf)
- **[VLMAE]** Vision-Language Masked Autoencoder [[pdf]](https://arxiv.org/pdf/2208.09374.pdf)
- **[MaskVLM]** Masked Vision and Language Modeling for Multi-modal Representation Learning [[pdf]](https://arxiv.org/pdf/2208.02131.pdf)
- **[MLIM]** Vision-and-Language Model Pre-training with Masked Language and Image Modeling [[pdf]](https://arxiv.org/pdf/2109.12178.pdf)
- **[MultiMAE]** Multi-modal Multi-task Masked Autoencoders [[pdf]](https://arxiv.org/pdf/2204.01678.pdf) [[code]](https://github.com/EPFL-VILAB/MultiMAE) [[website]](https://multimae.epfl.ch/) [[colab]](https://colab.research.google.com/github/EPFL-VILAB/MultiMAE/blob/main/MultiMAE_Demo.ipynb) [[demo]](https://huggingface.co/spaces/EPFL-VILAB/MultiMAE)
- **[OWL-ViT]** Simple Open-Vocabulary Object Detection with Vision Transformers [[pdf]](https://arxiv.org/pdf/2205.06230.pdf) [[code]](https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit) [[colab]](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/zeroshot_object_detection_with_owlvit.ipynb) [[demo]](https://huggingface.co/spaces/adirik/OWL-ViT) [[demo]](https://huggingface.co/spaces/adirik/image-guided-owlvit)
- Perceptual Grouping in Vision-Language Models [[pdf]](https://arxiv.org/pdf/2210.09996.pdf)
- Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers [[pdf]](https://arxiv.org/pdf/2109.04448.pdf)
- A Closer Look at the Robustness of Vision-and-Language Pre-trained Models [[pdf]](https://arxiv.org/pdf/2012.08673.pdf)
- Probing Inter-modality: Visual Parsing with Self-Attention for Vision-Language Pre-training [[pdf]](https://arxiv.org/pdf/2106.13488.pdf) 
- Multimodal Pretraining Unmasked: A Meta-Analysis and a Unified Framework of Vision-and-Language BERTs [[pdf]](https://arxiv.org/pdf/2011.15124.pdf) 
- Unsupervised Vision-and-Language Pre-training Without Parallel Images and Captions [[pdf]](https://arxiv.org/pdf/2010.12831.pdf)
- Cross-Modal Textual and Visual Context for Image Captioning [[pdf]](https://arxiv.org/pdf/2205.04363.pdf)
- Multi-modal Alignment using Representation Codebook [[pdf]](https://arxiv.org/pdf/2203.00048.pdf)
- A Closer Look at the Robustness of Vision-and-Language Pre-trained Models [[pdf]](https://arxiv.org/pdf/2012.08673.pdf)
- Multimodal Few-Shot Learning with Frozen Language Models [[pdf]](https://arxiv.org/pdf/2106.13884.pdf)
- On Guiding Visual Attention with Language Specification [[pdf]](https://arxiv.org/pdf/2202.08926.pdf)
- Compressing Visual-linguistic Model via Knowledge Distillation [[pdf]](https://arxiv.org/pdf/2104.02096.pdf)
- Playing Lottery Tickets with Vision and Language [[pdf]](https://arxiv.org/pdf/2104.11832.pdf)
- Visual Classification via Description from Large Language Models [[pdf]](https://arxiv.org/pdf/2210.07183.pdf)
- Do DALL-E and Flamingo Understand Each Other? [[pdf]](https://arxiv.org/pdf/2212.12249.pdf)

## Diffusion-based Vision-Language

- **[DALL-E-2]** Hierarchical Text-Conditional Image Generation with CLIP Latents [[pdf]](https://arxiv.org/pdf/2204.06125.pdf) [[code]](https://github.com/lucidrains/DALLE2-pytorch) [[website]](https://openai.com/dall-e-2/) [[blog]](http://adityaramesh.com/posts/dalle2/dalle2.html) [[blog]](https://www.assemblyai.com/blog/how-dall-e-2-actually-works/) [[blog]](https://medium.com/augmented-startups/how-does-dall-e-2-work-e6d492a2667f)
- **[Imagen]** Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding [[pdf]](https://arxiv.org/pdf/2205.11487.pdf) [[code]](https://github.com/lucidrains/imagen-pytorch) [[code]](https://github.com/cene555/Imagen-pytorch) [[website]](https://imagen.research.google/) [[video]](https://www.youtube.com/watch?v=xqDeAz0U-R4&ab_channel=AICoffeeBreakwithLetitia)
- **[Imagen Video]** High Definition Video Generation with Diffusion Models [[pdf]](https://imagen.research.google/video/paper.pdf)
- **[Stable Diffusion]** High-Resolution Image Synthesis with Latent Diffusion Models [[pdf]](https://arxiv.org/pdf/2112.10752.pdf) [[code]](https://github.com/CompVis/stable-diffusion) [[code]](https://github.com/CompVis/latent-diffusion) [[code]](https://github.com/lkwq007/stablediffusion-infinity) [[website]](https://ommer-lab.com/research/latent-diffusion-models/) [[demo]](https://huggingface.co/spaces/stabilityai/stable-diffusion) [[demo]](https://huggingface.co/spaces/multimodalart/stable-diffusion-inpainting) [[demo]](https://huggingface.co/spaces/lnyan/stablediffusion-infinity) [[demo]](https://huggingface.co/runwayml/stable-diffusion-v1-5) [[demo]](https://huggingface.co/runwayml/stable-diffusion-inpainting) [[demo]](https://huggingface.co/prompthero/midjourney-v4-diffusion) [[video]](https://www.youtube.com/watch?v=J87hffSMB60&ab_channel=AICoffeeBreakwithLetitia) [[video]](https://www.youtube.com/watch?v=f6PtJKdey8E&ab_channel=AleksaGordi%C4%87-TheAIEpiphany) [[video]](https://www.youtube.com/watch?v=ltLNYA3lWAQ&ab_channel=EdanMeyer) [[video]](https://www.youtube.com/watch?v=epktKtLWgHQ) [[colab]](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb) [[notebook]](https://github.com/multimodalart/latent-diffusion-notebook/blob/main/Latent_Diffusion_LAION_400M_model_text_to_image.ipynb) [[model card]](https://huggingface.co/CompVis/stable-diffusion-v1-4) [[organization card]](https://huggingface.co/CompVis) [[blog]](https://jalammar.github.io/illustrated-stable-diffusion/) [[blog]](https://towardsdatascience.com/how-to-fine-tune-stable-diffusion-using-textual-inversion-b995d7ecc095) [[blog]](https://huggingface.co/blog/stable_diffusion) [[finetuned diffusion]](https://huggingface.co/spaces/anzorq/finetuned_diffusion) [[finetuned diffusion]](https://huggingface.co/spaces/PublicPrompts/Pixel_diffusion) [[dpmsolver_sdm]](https://huggingface.co/spaces/LuChengTHU/dpmsolver_sdm) [[MAT Primer]](https://huggingface.co/spaces/Rothfeld/stable-diffusion-mat-outpainting-primer) [[retrieval-augmented]](https://arxiv.org/pdf/2204.11824.pdf) [[text-based inpainting]](https://huggingface.co/spaces/nielsr/text-based-inpainting)
- **[GLIDE]** Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models [[pdf]](https://arxiv.org/pdf/2112.10741.pdf) [[code]](https://github.com/openai/glide-text2im) [[video]](https://www.youtube.com/watch?v=gwI6g1pBD84&t=1976s&ab_channel=YannicKilcher) [[video]](https://www.youtube.com/watch?v=lvv4N2nf-HU&t=1891s&ab_channel=TheAIEpiphany) [[video]](https://www.youtube.com/watch?v=344w5h24-h8&t=5s&ab_channel=AICoffeeBreakwithLetitia)
- **[DiffusionCLIP]** Text-Guided Diffusion Models for Robust Image Manipulation [[pdf]](https://arxiv.org/pdf/2110.02711.pdf) [[code]](https://github.com/gwang-kim/DiffusionCLIP)
- **[VQ-Diffusion]** Vector Quantized Diffusion Model for Text-to-Image Synthesis [[pdf]](https://arxiv.org/pdf/2111.14822.pdf) [[code]](https://github.com/cientgu/VQ-Diffusion)
- **[clip2latent]** Text driven sampling of a pre-trained StyleGAN using denoising diffusion and CLIP [[pdf]](https://arxiv.org/pdf/2210.02347.pdf) [[code]](https://github.com/justinpinkney/clip2latent)
- **[DreamBooth]** Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation [[pdf]](https://arxiv.org/pdf/2208.12242v1.pdf) [[website]](https://dreambooth.github.io/) [[code]](https://github.com/XavierXiao/Dreambooth-Stable-Diffusion) [[code]](https://github.com/cantrell/Dreambooth-Stable-Diffusion-Tweaked)
- **[LDMs]** High-Resolution Image Synthesis with Latent Diffusion Models [[pdf]](https://arxiv.org/pdf/2112.10752.pdf) [[code]](https://github.com/CompVis/latent-diffusion)
- **[MagicMix]** Semantic Mixing with Diffusion Models [[pdf]](https://arxiv.org/pdf/2210.16056v1.pdf) [[website]](https://magicmix.github.io/)
- Prompt-to-Prompt Image Editing with Cross Attention Control [[pdf]](https://arxiv.org/pdf/2208.01626.pdf) [[code]](https://github.com/google/prompt-to-prompt)
- CLIP-Guided Diffusion [[code]](https://github.com/openai/guided-diffusion) [[code]](https://github.com/afiaka87/clip-guided-diffusion) [[code]](https://github.com/nerdyrodent/CLIP-Guided-Diffusion) [[code]](https://github.com/crowsonkb/v-diffusion-pytorch)

## CLIP-related
- **[CLIP]** Learning Transferable Visual Models From Natural Language Supervision [[pdf]](https://arxiv.org/pdf/2103.00020.pdf) [[code]](https://github.com/openai/CLIP) [[code]](https://github.com/Zasder3/train-CLIP) [[clip-retrieval]](https://github.com/rom1504/clip-retrieval) [[clip-retrieval blog]](https://rom1504.medium.com/semantic-search-with-embeddings-index-anything-8fb18556443c) [[CLIP_benchmark]](https://github.com/LAION-AI/CLIP_benchmark) [[website]](https://openai.com/blog/clip/) [[video]](https://www.youtube.com/watch?v=T9XSU0pKX2E&t=1455s&ab_channel=YannicKilcher) [[video code]](https://www.youtube.com/watch?v=jwZQD0Cqz4o&t=4610s&ab_channel=TheAIEpiphany)
- **[OpenCLIP]** Reproducible scaling laws for contrastive language-image learning [[pdf]](https://arxiv.org/pdf/2212.07143.pdf) [[code]](https://github.com/mlfoundations/open_clip) [[code]](https://github.com/LAION-AI/scaling-laws-openclip) [[colab]](https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_clip.ipynb)
- **[ALIGN]** Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision [[pdf]](https://arxiv.org/pdf/2102.05918.pdf)
- **[DeCLIP]** Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm [[pdf]](https://arxiv.org/pdf/2110.05208.pdf) [[code]](https://github.com/Sense-GVT/DeCLIP)
- **[LiT]** Zero-Shot Transfer with Locked-image text Tuning [[pdf]](https://arxiv.org/pdf/2111.07991.pdf) [[code]](https://github.com/google-research/vision_transformer) [[website]](https://google-research.github.io/vision_transformer/lit/)
- **[FLIP]** Scaling Language-Image Pre-training via Masking [[pdf]](https://arxiv.org/pdf/2212.00794.pdf)
- **[VT-CLIP]** Enhancing Vision-Language Models with Visual-guided Texts [[pdf]](https://arxiv.org/pdf/2112.02399.pdf)
- **[CLIP-ViL]** How Much Can CLIP Benefit Vision-and-Language Tasks? [[pdf]](https://arxiv.org/pdf/2107.06383.pdf) [[code]](https://github.com/clip-vil/CLIP-ViL)
- **[RegionCLIP]** Region-based Language-Image Pretraining [[pdf]](https://arxiv.org/pdf/2112.09106.pdf) [[code]](https://github.com/microsoft/RegionCLIP)
- **[DenseCLIP]** Language-Guided Dense Prediction with Context-Aware Prompting [[pdf]](https://arxiv.org/pdf/2112.01518.pdf) [[code]](https://github.com/raoyongming/DenseCLIP)
- **[E-CLIP]** Efficient Cross-Modal Pre-training by Ensemble Confident Learning and Language Modeling [[pdf]](https://arxiv.org/pdf/2109.04699.pdf)
- **[X-CLIP]** End-to-End Multi-grained Contrastive Learning for Video-Text Retrieval [[pdf]](https://arxiv.org/pdf/2207.07285.pdf) [[code]](https://github.com/xuguohai/X-CLIP)
- **[MaskCLIP]** Masked Self-Distillation Advances Contrastive Language-Image Pretraining [[pdf]](https://arxiv.org/pdf/2208.12262.pdf)
- **[CLIPSeg]** Image Segmentation Using Text and Image Prompts [[pdf]](https://arxiv.org/pdf/2112.10003.pdf) [[blog]](https://huggingface.co/blog/clipseg-zero-shot) [[code]](https://github.com/timojl/clipseg) [[demo]](https://huggingface.co/spaces/nielsr/CLIPSeg) [[colab]](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/123_clipseg-zero-shot.ipynb)
- **[ClipCap]**  CLIP Prefix for Image Captioning [[pdf]](https://arxiv.org/pdf/2111.09734.pdf) [[code]](https://github.com/rmokady/CLIP_prefix_caption) [[code]](https://github.com/TheoCoombes/ClipCap)
- **[VQGAN-CLIP]** Open Domain Image Generation and Editing with Natural Language Guidance [[pdf]](https://arxiv.org/pdf/2204.08583.pdf) [[code]](https://github.com/nerdyrodent/VQGAN-CLIP) [[code]](https://github.com/EleutherAI/vqgan-clip) [[code]](https://github.com/justinjohn0306/VQGAN-CLIP) [[code]](https://www.kaggle.com/code/basu369victor/playing-with-vqgan-clip/notebook) [[colab]](https://colab.research.google.com/github/dribnet/clipit/blob/master/demos/Moar_Settings.ipynb) [[colab]](https://colab.research.google.com/drive/1L8oL-vLJXVcRzCFbPwOoMkPKJ8-aYdPN) [[colab]](https://colab.research.google.com/github/justinjohn0306/VQGAN-CLIP/blob/main/VQGAN%2BCLIP(Updated).ipynb)
- **[Paella]** Fast Text-Conditional Discrete Denoising on Vector-Quantized Latent Spaces [[pdf]](https://arxiv.org/pdf/2211.07292.pdf) [[code]](https://github.com/dome272/Paella) [[video]](https://www.youtube.com/watch?v=6zeLSANd41k&ab_channel=AICoffeeBreakwithLetitia)
- **[CLIPPO]** Image-and-Language Understanding from Pixels Only [[pdf]](https://arxiv.org/pdf/2212.08045.pdf)
- **[SVLC]** Teaching Structured Vision & Language Concepts to Vision & Language Models [[pdf]](https://arxiv.org/pdf/2211.11733.pdf)
- **[CuPL]** Generating customized prompts for zero-shot image classification [[pdf]](https://arxiv.org/pdf/2209.03320.pdf) [[code]](https://github.com/sarahpratt/CuPL)
- Robust Cross-Modal Representation Learning with Progressive Self-Distillation [[pdf]](https://arxiv.org/pdf/2204.04588.pdf)
- Disentangling visual and written concepts in CLIP [[pdf]](https://arxiv.org/pdf/2206.07835.pdf) [[code]](https://github.com/joaanna/disentangling_spelling_in_clip) [[website]](https://joaanna.github.io/disentangling_spelling_in_clip/)
- CLIP Models are Few-shot Learners: Empirical Studies on VQA and Visual Entailment [[pdf]](https://arxiv.org/pdf/2203.07190.pdf)
- CLIP Itself is a Strong Fine-tuner: Achieving 85.7% and 88.0% Top-1 Accuracy with ViT-B and ViT-L on ImageNet [[pdf]](https://arxiv.org/pdf/2212.06138.pdf) [[code]](https://github.com/LightDXY/FT-CLIP)
- [CLIP Varaints](https://github.com/lucidrains/x-clip)
- [awesome-clip](https://github.com/yzhuoning/Awesome-CLIP)

## New Large-Scale Datasets
- **[VisualCOMET]** Reasoning about the Dynamic Context of a Still Image [[pdf]](https://arxiv.org/pdf/2004.10796.pdf) [[website]](https://visualcomet.xyz/)
- **[LAION]** [[website]](https://laion.ai/) [[hf website]](https://huggingface.co/laion) [[paper]](https://openreview.net/pdf?id=M3Y74vmsMcY) [[paper]](https://arxiv.org/pdf/2111.02114.pdf) [[img2dataset]](https://github.com/rom1504/img2dataset)
- **[Conceptual 12M]** Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts [[pdf]](https://arxiv.org/pdf/2102.08981.pdf) [[code]](https://github.com/google-research-datasets/conceptual-12m)
- **[Winoground]** Probing Vision and Language Models for Visio-Linguistic Compositionality [[pdf]](https://arxiv.org/pdf/2204.03162.pdf) [[dataset]](https://huggingface.co/datasets/facebook/winoground)

### Libraries
- LAVIS [[github]](https://github.com/salesforce/LAVIS) [[docs]](https://opensource.salesforce.com/LAVIS//latest/index.html#)
- Diffusers [[github]](https://github.com/huggingface/diffusers) [[docs]](https://huggingface.co/docs/diffusers/index)
- X-modaler [[github]](https://github.com/YehLi/xmodaler) [[docs]](https://xmodaler.readthedocs.io/en/latest/index.html)
- MMF [[github]](https://github.com/facebookresearch/mmf) [[docs]](https://mmf.sh/docs/)
- TorchMultimodal [[github]](https://github.com/facebookresearch/multimodal) [[blog]](https://pytorch.org/blog/introducing-torchmultimodal/) [[blog]](https://pytorch.org/blog/scaling-multimodal-foundation-models-in-torchmultimodal-with-pytorch-distributed/)
- [Transformers-VQA](https://github.com/YIKUAN8/Transformers-VQA)
- [MMT-Retrieval](https://github.com/UKPLab/MMT-Retrieval)

### Projects
- [Florence-VL](https://www.microsoft.com/en-us/research/project/project-florence-vl/)
- [unilm](https://github.com/microsoft/unilm)

### Other Awesomes
- [awesome-Vision-and-Language-Pre-training](https://github.com/phellonchen/awesome-Vision-and-Language-Pre-training)
- [awesome-vision-language-pretraining-papers](https://github.com/yuewang-cuhk/awesome-vision-language-pretraining-papers)
- [vqa](https://github.com/jokieleung/awesome-visual-question-answering)
- [image captioning](https://github.com/forence/Awesome-Visual-Captioning)
- [image captioning](https://github.com/zhjohnchan/awesome-image-captioning)
- [scene graphs](https://github.com/huoxingmeishi/Awesome-Scene-Graphs)

### Surveys
- A Survey of Vision-Language Pre-Trained Models [[pdf]](https://arxiv.org/pdf/2202.10936.pdf)
- VLP: A Survey on Vision-Language Pre-training [[pdf]](https://arxiv.org/pdf/2202.09061.pdf)
- Vision Language models: towards multi-modal deep learning [[blog]](https://theaisummer.com/vision-language-models/)

### Resources
- [Generalized Visual Language Models](https://lilianweng.github.io/posts/2022-06-09-vlm/)
- [CVPR22 Tutorial](https://vlp-tutorial.github.io/2022/)
- [CVPR21 Tutorial](https://vqa2vln-tutorial.github.io/)
- [CVPR20 Tutorial](https://rohit497.github.io/Recent-Advances-in-Vision-and-Language-Research/)

